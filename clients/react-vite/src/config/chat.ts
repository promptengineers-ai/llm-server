import { ModelType, EmbeddingModel, SearchProvider, SearchType } from "@/types/llm";
import { ON_PREM } from "./app";

export const defaultState = {
    chatInputRef: null,
    chatboxRef: null,
    userInputRef: null,
    responseRef: "",
    response: "",
    userInput: "",
    models: [],
    chats: [],
    messages: [],
    images: [],
    files: [],
    actions: [],
    logs: [],
    tools: [],
    indexes: [],
    loaders: [],
    expand: false,
    done: true,
    selectedImage: null,
    selectedDocument: null,
    csvContent: null,
    isSaveEnabled: false,
    selectedModel: ON_PREM ? ModelType.OLLAMA_LLAMA_3_CHAT : ModelType.OPENAI_GPT_4_OMNI,
    chatPayload: {
        query: "",
        history_id: "",
        system: "You are a helpful AI assistant.",
        model: ON_PREM ? ModelType.OLLAMA_LLAMA_3_CHAT : ModelType.OPENAI_GPT_4_OMNI,
        temperature: 0.5,
        tools: [],
        retrieval: {
            provider: SearchProvider.POSTGRES,
            embedding: ON_PREM ? EmbeddingModel.OLLAMA_NOMIC_EMBED_TEXT : EmbeddingModel.OPENAI_TEXT_EMBED_3_LARGE,
            index_name: "",
            indexes: [],
            search_type: SearchType.MMR,
            search_kwargs: {
                k: 10,
                fetch_k: null,
                score_threshold: null,
            },
            batch_size: 100,
            parallel: false,
            workers: 4,
        },
    },
}; 